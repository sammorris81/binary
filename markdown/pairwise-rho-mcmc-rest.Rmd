---
title: "Another approach"
author: "Morris, S."
date: "June 2, 2015"
output: pdf_document
---

```{r setup, echo=FALSE, include=FALSE}
# libraries
library(fields)
library(Rcpp)
library(evd)
library(spBayes)
library(mvtnorm)
Sys.setenv("PKG_CXXFLAGS"="-fopenmp")
Sys.setenv("PKG_LIBS"="-fopenmp")
sourceCpp(file = "../code/R/pairwise.cpp")

source("../code/R/auxfunctions.R", chdir = TRUE)
source("../code/R/updateModel.R")
source("../code/R/mcmc.R")
source("../code/R/probit.R", chdir=T)

# true knots
knots.t <- as.matrix(expand.grid(seq(0.00, 1.00, length=12), 
                                 seq(0.00, 1.00, length=12)))

# knots used to fit model
knots <- as.matrix(expand.grid(seq(0.00, 1.00, length=15), 
                               seq(0.00, 1.00, length=15))) 
```

```{r setting-1, echo=FALSE, include=FALSE}
# get sites and distances from knots
set.seed(7483)  # sites
ns  <- 1500
s   <- cbind(runif(ns), runif(ns))
dw2 <- rdist(s, knots)
d   <- as.matrix(rdist(s))
diag(d) <- 0
x   <- matrix(1, ns, 1)

# only focusing on the case of strong dependence
alpha.t <- 0.35
alpha.min <- 0
alpha.max <- 1
alpha.rng <- alpha.max - alpha.min
xi.t    <- 0.25
rho.t   <- 0.15
prop    <- c(0.05, 0.01)
knots.h <- knots[2, 1] - knots[1, 1]
rho.init <- log(knots.h)
```

```{r mcmcsetup, echo=FALSE, include=FALSE}
# y is ns, nt, nsets, nsettings
iters <- 40000; burn <- 30000; update <- 500; thin <- 1

# setup for spGLM
n.report <- 500
verbose <- TRUE
tuning <- list("phi"=0.1, "sigma.sq"=0.1, "tau.sq"=0.1,
               "beta"=0.1, "w"=0.1)
starting <- list("phi"=3/0.5, "sigma.sq"=50, "tau.sq"=1,
                 "beta"=0, "w"=0)
priors <- list("beta.norm"=list(1, 100),
               "phi.unif"=c(0.1, 1e4), "sigma.sq.ig"=c(1, 1),
               "tau.sq.ig"=c(1, 1))
cov.model <- "exponential"
```

# Current challenges

So far, I have not been able to really find a good approach that works consistently.
Brian and I had originally discussed fixing both $\rho$ and $\alpha$ in the simulation study, because when they're fixed to the true values, we can outperform spatial probit and logit.
There appears to be some challenges when trying to estimate $\alpha$ using the pairwise likelihood, but it would appear that the pairwise likelihood does a reasonably good job estimating the bandwidth term $\rho$.
I've been spending time trying to better understand how this will impact the results from the MCMC, and overall, I think the impact of fixing $\alpha$ (even if the value is not correct) will be minimal.

## The role of $\alpha$

One of the roles of $\alpha$ in the conditional likelihood for the MCMC (i.e. conditional on knowing $\theta$) is to allow for more variability in the positive stable random variables.
As $\alpha \rightarrow 0$, var$(A) \nearrow$, and as $\alpha \rightarrow 1$, the density converges to a point mass at 1.
In the binary framework, I do not think it is important that we get the actual intensity of the $A$ terms correct.
However, we do want to ensure that we can maintain the relative intensities of the knots.
While running some test cases of MCMC, it seems as if $\alpha = 0.5$ would result in a sequence of knots like $\{1, 4, 5, 2, 8}$ whereas $\alpha = 0.2$ would result in a sequence of knots like $\{100, 400, 500, 200, 800\}$. 

# Goal

This document explores two changes to how we're conducting the analysis.
The first change is to fix $\alpha$ and $\rho$ to the estimates from the pairwise composite likelihood.
To estimate rho, we do a grid search using `optim` at 5 different values of $\rho$ based on knot spacing.
Then we take the value for $\rho$ that minimizes the negative log pairwise composite likelihood (PCL).
The second change is to try a new candidate distribution for the $\beta$ and $\xi$ terms.
Initially, we just used a random walk metropolis step for each term.
In this update, we are using a candidate distribution based on the estimates and hessian matrix from the PCL.
We will be drawing candidates for $\beta$ and $\xi$ using a multivariate $t$ distribution with $\nu = 15$ degrees of freedom that has been shifted to $(\widehat{\beta}, \widehat{xi}$, and has dispersion matrix $H^{-1}$ where $H$ is the hessian matrix from the PCL.
Although there may be some concerns about fully exploring the posterior density, Brian and I thought this would be a reasonable way to work around convergence issues that present themselves when using a random walk.

## Setting 1:

$\alpha = `r alpha.t`, \pi = `r prop[1]`, \rho = `r rho.t`$

### Dataset 1

```{r generate1-1, echo=FALSE, include=FALSE}
data.seed <- 3282
set.seed(data.seed)  # data
data <- rRareBinarySpat(x, s = s, knots = knots.t, beta = 0, xi = xi.t,
                        alpha = alpha.t, rho = rho.t, prob.success = prop[1])
y <- data$y

ntrain <- floor(0.75 * ns)
ntest  <- ns - ntrain
obs   <- c(rep(T, ntrain), rep(F, ntest))
y.o   <- y[obs, , drop = FALSE]
X.o   <- matrix(x[obs], ntrain, 1)
s.o   <- s[obs, ]
dw2.o <- rdist(s.o, knots)
d.o   <- as.matrix(rdist(s.o))
diag(d.o) <- 0
y.validate <- y[!obs, , drop = FALSE]
X.p <- matrix(x[!obs, ], ntest, 1)
s.p <- s[!obs, ]
```

```{r fit1-1, echo=FALSE, include=FALSE, cache=TRUE}
fit <- fit.rarebinaryCPP(c(0, rho.init, 0, -4), y = y.o, dw2 = dw2.o, d = d.o, 
                         cov=X.o, alpha.min = alpha.min, alpha.max = alpha.max, 
                         threads=6)

alpha.hat  <- (exp(fit$par[1]) / (1 + exp(fit$par[1]))) * 
               alpha.rng + alpha.min
if (alpha.hat < 0.3) { 
  alpha.hat <- 0.3
} else if (alpha.hat > 0.9) {
  alpha.hat <- 0.9
}
rho.hat    <- exp(fit$par[2])
xibeta.hat <- fit$par[3:4]
xibeta.var <- solve(fit$hessian[3:4, 3:4])
```

From the pairwise likelihood, we'll be using $\rho = `r round(fit$par[2], 4)`$.
The estimates for the other parameters are $\widehat{\alpha} = `r round(fit$par[1], 3)`$, $\widehat{\xi} = `r round(fit$par[2], 3)`$, and $\widehat{\beta}_0 = `r round(fit$par[3], 3)`$.

```{r plotdata1-1, echo=FALSE, fig.width=4, fig.height=4}
plot(knots.t, ylim = c(0, 1), xlim = c(0, 1), 
     main = "simulated dataset", xlab="", ylab="")
train1.idx <- which(y[obs] == 1)
test1.idx <- which(y[!obs] == 1) + ntrain  # to get to the testing
points(s[train1.idx, ], pch = 21, col = "dodgerblue4", bg = "dodgerblue1")
points(s[test1.idx, ], pch = 21, col = "firebrick4", bg = "firebrick1")
```
 
```{r logit1-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial logit
mcmc.seed <- 101
set.seed(mcmc.seed)
fit.logit <- spGLM(formula = y.o ~ 1, family = "binomial", coords = s.o,
                   knots = knots, starting = starting, tuning = tuning,
                   priors = priors, cov.model = cov.model,
                   n.samples = iters, verbose = verbose,
                   n.report = n.report)
```
 
```{r probit1-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial probit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.probit <- probit(Y = y.o, X = X.o, s = s.o, knots = knots,
                     iters = iters, burn = burn, update = update)
```
 
```{r gev1-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                knots = knots, beta.tune = 1, xi.tune = 0.1,
                alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                beta.attempts = 200, xi.attempts = 200,
                alpha.attempts = 7500, rho.attempts = 100,
                spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                alpha.init = alpha.hat, a.init = 10000, iterplot = FALSE,
                alpha.fix = FALSE, rho.fix = TRUE, xibeta.joint = TRUE,
                xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev1-1-g1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g1 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.3, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev1-1-g2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g2 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.5, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev1-1-g3, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g3 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.7, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

## MCMC Results

Here are the iteration plots from the GEV model. 
The true values are $\beta_0 = `r round(-data$thresh, 3)`$, and $\xi = `r xi.t`$.
This is using $\widehat{\alpha}$ and $\widehat{\rho}$ from the PCL fit.

```{r gev1ters1-1, echo=FALSE, fig.width=7, fig.height=2}
par(mfrow=c(1, 2))
plot(fit.gev$beta, type="l", main=bquote(beta[0]))
plot(fit.gev$xi, type="l", main=bquote(xi))
```

```{r resultslogit1-1, echo=FALSE, include=FALSE, cache=TRUE}
yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.p,
                       pred.covars = X.p, start = burn + 1, end = iters,
                       thin = 1, verbose = TRUE, n.report = 500)

post.prob.log <- t(yp.sp.log$p.y.predictive.samples)
bs.log1.1        <- BrierScore(post.prob.log, y.validate)
```

```{r resultsprobit1-1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,
                             s.pred = s.p, knots = knots,
                             start = 1, end = iters - burn, update = 500)
bs.pro1.1 <- BrierScore(post.prob.pro, y.validate)
```

```{r resultsgev1-1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.1.1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev1-1-g1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g1, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.1.g1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev1-1-g2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g2, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.1.g2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev1-1-g3, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g3, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.1.g3 <- BrierScore(post.prob.gev, y.validate)
```

### Dataset 2

```{r generate2-1, echo=FALSE, include=FALSE}
data.seed <- data.seed + 1
set.seed(data.seed)  # data
data <- rRareBinarySpat(x, s = s, knots = knots.t, beta = 0, xi = xi.t,
                        alpha = alpha.t, rho = rho.t, prob.success = prop[1])
y <- data$y

ntrain <- floor(0.75 * ns)
ntest  <- ns - ntrain
obs   <- c(rep(T, ntrain), rep(F, ntest))
y.o   <- y[obs, , drop = FALSE]
X.o   <- matrix(x[obs], ntrain, 1)
s.o   <- s[obs, ]
dw2.o <- rdist(s.o, knots)
d.o   <- as.matrix(rdist(s.o))
diag(d.o) <- 0
y.validate <- y[!obs, , drop = FALSE]
X.p <- matrix(x[!obs, ], ntest, 1)
s.p <- s[!obs, ]
```

```{r fit2-1, echo=FALSE, include=FALSE, cache=TRUE}
fit <- fit.rarebinaryCPP(c(0, rho.init, 0, -4), y = y.o, dw2 = dw2.o, d = d.o, 
                         cov=X.o, alpha.min = alpha.min, alpha.max = alpha.max, 
                         threads=6)

alpha.hat  <- (exp(fit$par[1]) / (1 + exp(fit$par[1]))) * 
               alpha.rng + alpha.min
if (alpha.hat < 0.3) { 
  alpha.hat <- 0.3
} else if (alpha.hat > 0.9) {
  alpha.hat <- 0.9
}
rho.hat    <- exp(fit$par[2])
xibeta.hat <- fit$par[3:4]
xibeta.var <- solve(fit$hessian[3:4, 3:4])
```

From the pairwise likelihood, we'll be using $\rho = `r round(fit$par[2], 4)`$.
The estimates for the other parameters are $\widehat{\alpha} = `r round(fit$par[1], 3)`$, $\widehat{\xi} = `r round(fit$par[2], 3)`$, and $\widehat{\beta}_0 = `r round(fit$par[3], 3)`$.

```{r plotdata2-1, echo=FALSE, fig.width=4, fig.height=4}
plot(knots.t, ylim = c(0, 1), xlim = c(0, 1), 
     main = "simulated dataset", xlab="", ylab="")
train1.idx <- which(y[obs] == 1)
test1.idx <- which(y[!obs] == 1) + ntrain  # to get to the testing
points(s[train1.idx, ], pch = 21, col = "dodgerblue4", bg = "dodgerblue1")
points(s[test1.idx, ], pch = 21, col = "firebrick4", bg = "firebrick1")
```
 
```{r logit2-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial logit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.logit <- spGLM(formula = y.o ~ 1, family = "binomial", coords = s.o,
                   knots = knots, starting = starting, tuning = tuning,
                   priors = priors, cov.model = cov.model,
                   n.samples = iters, verbose = verbose,
                   n.report = n.report)
```
 
```{r probit2-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial probit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.probit <- probit(Y = y.o, X = X.o, s = s.o, knots = knots,
                     iters = iters, burn = burn, update = update)
```
 
```{r gev2-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                knots = knots, beta.tune = 1, xi.tune = 0.1,
                alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                beta.attempts = 200, xi.attempts = 200,
                alpha.attempts = 7500, rho.attempts = 100,
                spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                alpha.init = alpha.hat, a.init = 10000, iterplot = FALSE,
                alpha.fix = FALSE, rho.fix = TRUE, xibeta.joint = TRUE,
                xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev2-1-g1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g1 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.3, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev2-1-g2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g2 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.5, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev2-1-g3, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g3 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.7, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

## MCMC Results

Here are the iteration plots from the GEV model. 
The true values are $\beta_0 = `r round(-data$thresh, 3)`$, and $\xi = `r xi.t`$.
This is using $\widehat{\alpha}$ and $\widehat{\rho}$ from the PCL fit.

```{r gev1ters2-1, echo=FALSE, fig.width=7, fig.height=2}
par(mfrow=c(1, 2))
plot(fit.gev$beta, type="l", main=bquote(beta[0]))
plot(fit.gev$xi, type="l", main=bquote(xi))
```

```{r resultslogit2-1, echo=FALSE, include=FALSE, cache=TRUE}
yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.p,
                       pred.covars = X.p, start = burn + 1, end = iters,
                       thin = 1, verbose = TRUE, n.report = 500)

post.prob.log <- t(yp.sp.log$p.y.predictive.samples)
bs.log2.1        <- BrierScore(post.prob.log, y.validate)
```

```{r resultsprobit2-1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,
                             s.pred = s.p, knots = knots,
                             start = 1, end = iters - burn, update = 500)
bs.pro2.1 <- BrierScore(post.prob.pro, y.validate)
```

```{r resultsgev2-1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev2-1-g1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g1, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.1.g1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev2-1-g2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g2, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.1.g2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev2-1-g3, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g3, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.1.g3 <- BrierScore(post.prob.gev, y.validate)
```

### Dataset 3

```{r generate3-1, echo=FALSE, include=FALSE}
data.seed <- data.seed + 1
set.seed(data.seed)  # data
data <- rRareBinarySpat(x, s = s, knots = knots.t, beta = 0, xi = xi.t,
                        alpha = alpha.t, rho = rho.t, prob.success = prop[1])
y <- data$y

ntrain <- floor(0.75 * ns)
ntest  <- ns - ntrain
obs   <- c(rep(T, ntrain), rep(F, ntest))
y.o   <- y[obs, , drop = FALSE]
X.o   <- matrix(x[obs], ntrain, 1)
s.o   <- s[obs, ]
dw2.o <- rdist(s.o, knots)
d.o   <- as.matrix(rdist(s.o))
diag(d.o) <- 0
y.validate <- y[!obs, , drop = FALSE]
X.p <- matrix(x[!obs, ], ntest, 1)
s.p <- s[!obs, ]
```

```{r fit3-1, echo=FALSE, include=FALSE, cache=TRUE}
fit <- fit.rarebinaryCPP(c(0, rho.init, 0, -4), y = y.o, dw2 = dw2.o, d = d.o, 
                         cov=X.o, alpha.min = alpha.min, alpha.max = alpha.max, 
                         threads=6)

alpha.hat  <- (exp(fit$par[1]) / (1 + exp(fit$par[1]))) * 
               alpha.rng + alpha.min
if (alpha.hat < 0.3) { 
  alpha.hat <- 0.3
} else if (alpha.hat > 0.9) {
  alpha.hat <- 0.9
}
rho.hat    <- exp(fit$par[2])
xibeta.hat <- fit$par[3:4]
xibeta.var <- solve(fit$hessian[3:4, 3:4])
```

From the pairwise likelihood, we'll be using $\rho = `r round(fit$par[2], 4)`$.
The estimates for the other parameters are $\widehat{\alpha} = `r round(fit$par[1], 3)`$, $\widehat{\xi} = `r round(fit$par[2], 3)`$, and $\widehat{\beta}_0 = `r round(fit$par[3], 3)`$.

```{r plotdata3-1, echo=FALSE, fig.width=4, fig.height=4}
plot(knots.t, ylim = c(0, 1), xlim = c(0, 1), 
     main = "simulated dataset", xlab="", ylab="")
train1.idx <- which(y[obs] == 1)
test1.idx <- which(y[!obs] == 1) + ntrain  # to get to the testing
points(s[train1.idx, ], pch = 21, col = "dodgerblue4", bg = "dodgerblue1")
points(s[test1.idx, ], pch = 21, col = "firebrick4", bg = "firebrick1")
```
 
```{r logit3-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial logit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.logit <- spGLM(formula = y.o ~ 1, family = "binomial", coords = s.o,
                   knots = knots, starting = starting, tuning = tuning,
                   priors = priors, cov.model = cov.model,
                   n.samples = iters, verbose = verbose,
                   n.report = n.report)
```
 
```{r probit3-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial probit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.probit <- probit(Y = y.o, X = X.o, s = s.o, knots = knots,
                     iters = iters, burn = burn, update = update)
```


```{r gev3-1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                knots = knots, beta.tune = 1, xi.tune = 0.1,
                alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                beta.attempts = 200, xi.attempts = 200,
                alpha.attempts = 7500, rho.attempts = 100,
                spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                alpha.init = alpha.hat, a.init = 10000, iterplot = FALSE,
                alpha.fix = FALSE, rho.fix = TRUE, xibeta.joint = TRUE,
                xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev3-1-g1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g1 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.3, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev3-1-g2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g2 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.5, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev3-1-g3, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g3 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.7, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

## MCMC Results

Here are the iteration plots from the GEV model. 
The true values are $\beta_0 = `r round(-data$thresh, 3)`$, and $\xi = `r xi.t`$.
This is using $\widehat{\alpha}$ and $\widehat{\rho}$ from the PCL fit.

```{r gev1ters3-1, echo=FALSE, fig.width=7, fig.height=2}
par(mfrow=c(1, 2))
plot(fit.gev$beta, type="l", main=bquote(beta[0]))
plot(fit.gev$xi, type="l", main=bquote(xi))
```

```{r resultslogit3-1, echo=FALSE, include=FALSE, cache=TRUE}
yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.p,
                       pred.covars = X.p, start = burn + 1, end = iters,
                       thin = 1, verbose = TRUE, n.report = 500)

post.prob.log <- t(yp.sp.log$p.y.predictive.samples)
bs.log3.1        <- BrierScore(post.prob.log, y.validate)
```

```{r resultsprobit3-1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,
                             s.pred = s.p, knots = knots,
                             start = 1, end = iters - burn, update = 500)
bs.pro3.1 <- BrierScore(post.prob.pro, y.validate)
```

```{r resultsgev3-1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev3-1-g1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g1, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.1.g1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev3-1-g2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g2, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.1.g2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev3-1-g3, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g3, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.1.g3 <- BrierScore(post.prob.gev, y.validate)
```

## Brier Scores

The brier scores are 

Logit 1-1: `r round(bs.log1.1, 4)`

Probit 1-1: `r round(bs.pro1.1, 4)`

GEV 1-1: `r round(bs.gev1.1, 4)`

GEV 1-1-g1: `r round(bs.gev1.1.g1, 4)`

GEV 1-1-g2: `r round(bs.gev1.1.g2, 4)`

GEV 1-1-g3: `r round(bs.gev1.1.g3, 4)`

The brier scores are 

Logit 2-1: `r round(bs.log2.1, 4)`

Probit 2-1: `r round(bs.pro2.1, 4)`

GEV 2-1: `r round(bs.gev2.1, 4)`

GEV 2-1-g1: `r round(bs.gev2.1.g1, 4)`

GEV 2-1-g2: `r round(bs.gev2.1.g2, 4)`

GEV 2-1-g3: `r round(bs.gev2.1.g3, 4)`

The brier scores are 

Logit 3-1: `r round(bs.log3.1, 4)`

Probit 3-1: `r round(bs.pro3.1, 4)`

GEV 3-1: `r round(bs.gev3.1, 4)`

GEV 3-1-g1: `r round(bs.gev3.1.g1, 4)`

GEV 3-1-g2: `r round(bs.gev3.1.g2, 4)`

GEV 3-1-g3: `r round(bs.gev3.1.g3, 4)`

## Setting 2:

$\alpha = `r alpha.t`, \pi = `r prop[2]`, \rho = `r rho.t`$

### Dataset 1

```{r generate1-2, echo=FALSE, include=FALSE}
data.seed <- data.seed + 1
set.seed(data.seed)  # data
data <- rRareBinarySpat(x, s = s, knots = knots.t, beta = 0, xi = xi.t,
                        alpha = alpha.t, rho = rho.t, prob.success = prop[2])
y <- data$y

ntrain <- floor(0.75 * ns)
ntest  <- ns - ntrain
obs   <- c(rep(T, ntrain), rep(F, ntest))
y.o   <- y[obs, , drop = FALSE]
X.o   <- matrix(x[obs], ntrain, 1)
s.o   <- s[obs, ]
dw2.o <- rdist(s.o, knots)
d.o   <- as.matrix(rdist(s.o))
diag(d.o) <- 0
y.validate <- y[!obs, , drop = FALSE]
X.p <- matrix(x[!obs, ], ntest, 1)
s.p <- s[!obs, ]
```

```{r fit1-2, echo=FALSE, include=FALSE, cache=TRUE}
fit <- fit.rarebinaryCPP(c(0, rho.init, 0, -4), y = y.o, dw2 = dw2.o, d = d.o, 
                         cov=X.o, alpha.min = alpha.min, alpha.max = alpha.max, 
                         threads=6)

alpha.hat  <- (exp(fit$par[1]) / (1 + exp(fit$par[1]))) * 
               alpha.rng + alpha.min
if (alpha.hat < 0.3) { 
  alpha.hat <- 0.3
} else if (alpha.hat > 0.9) {
  alpha.hat <- 0.9
}
rho.hat    <- exp(fit$par[2])
xibeta.hat <- fit$par[3:4]
xibeta.var <- solve(fit$hessian[3:4, 3:4])
```

From the pairwise likelihood, we'll be using $\rho = `r round(fit$par[2], 4)`$.
The estimates for the other parameters are $\widehat{\alpha} = `r round(fit$par[1], 3)`$, $\widehat{\xi} = `r round(fit$par[2], 3)`$, and $\widehat{\beta}_0 = `r round(fit$par[3], 3)`$.

```{r plotdata1-2, echo=FALSE, fig.width=4, fig.height=4}
plot(knots.t, ylim = c(0, 1), xlim = c(0, 1), 
     main = "simulated dataset", xlab="", ylab="")
train1.idx <- which(y[obs] == 1)
test1.idx <- which(y[!obs] == 1) + ntrain  # to get to the testing
points(s[train1.idx, ], pch = 21, col = "dodgerblue4", bg = "dodgerblue1")
points(s[test1.idx, ], pch = 21, col = "firebrick4", bg = "firebrick1")
```
 
```{r logit1-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial logit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.logit <- spGLM(formula = y.o ~ 1, family = "binomial", coords = s.o,
                   knots = knots, starting = starting, tuning = tuning,
                   priors = priors, cov.model = cov.model,
                   n.samples = iters, verbose = verbose,
                   n.report = n.report)
```
 
```{r probit1-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial probit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.probit <- probit(Y = y.o, X = X.o, s = s.o, knots = knots,
                     iters = iters, burn = burn, update = update)
```
 
```{r gev1-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                knots = knots, beta.tune = 1, xi.tune = 0.1,
                alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                beta.attempts = 200, xi.attempts = 200,
                alpha.attempts = 7500, rho.attempts = 100,
                spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                alpha.init = alpha.hat, a.init = 10000, iterplot = FALSE,
                alpha.fix = FALSE, rho.fix = TRUE, xibeta.joint = TRUE,
                xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev1-2-g1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g1 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.3, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev1-2-g2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g2 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.5, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev1-2-g3, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g3 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.7, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

## MCMC Results

Here are the iteration plots from the GEV model. 
The true values are $\beta_0 = `r round(-data$thresh, 3)`$, and $\xi = `r xi.t`$.
This is using $\widehat{\alpha}$ and $\widehat{\rho}$ from the PCL fit.

```{r gev1ters1-2, echo=FALSE, fig.width=7, fig.height=2}
par(mfrow=c(1, 2))
plot(fit.gev$beta, type="l", main=bquote(beta[0]))
plot(fit.gev$xi, type="l", main=bquote(xi))
```

```{r resultslogit1-2, echo=FALSE, include=FALSE, cache=TRUE}
yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.p,
                       pred.covars = X.p, start = burn + 1, end = iters,
                       thin = 1, verbose = TRUE, n.report = 500)

post.prob.log <- t(yp.sp.log$p.y.predictive.samples)
bs.log1.2        <- BrierScore(post.prob.log, y.validate)
```

```{r resultsprobit1-2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,
                             s.pred = s.p, knots = knots,
                             start = 1, end = iters - burn, update = 500)
bs.pro1.2 <- BrierScore(post.prob.pro, y.validate)
```

```{r resultsgev1-2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev1-2-g1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g1, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.2.g1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev1-2-g2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g2, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.2.g2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev1-2-g3, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g3, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev1.2.g3 <- BrierScore(post.prob.gev, y.validate)
```

### Dataset 2

```{r generate2-2, echo=FALSE, include=FALSE}
data.seed <- data.seed + 1
set.seed(data.seed)  # data
data <- rRareBinarySpat(x, s = s, knots = knots.t, beta = 0, xi = xi.t,
                        alpha = alpha.t, rho = rho.t, prob.success = prop[2])
y <- data$y

ntrain <- floor(0.75 * ns)
ntest  <- ns - ntrain
obs   <- c(rep(T, ntrain), rep(F, ntest))
y.o   <- y[obs, , drop = FALSE]
X.o   <- matrix(x[obs], ntrain, 1)
s.o   <- s[obs, ]
dw2.o <- rdist(s.o, knots)
d.o   <- as.matrix(rdist(s.o))
diag(d.o) <- 0
y.validate <- y[!obs, , drop = FALSE]
X.p <- matrix(x[!obs, ], ntest, 1)
s.p <- s[!obs, ]
```

```{r fit2-2, echo=FALSE, include=FALSE, cache=TRUE}
fit <- fit.rarebinaryCPP(c(0, rho.init, 0, -4), y = y.o, dw2 = dw2.o, d = d.o, 
                         cov=X.o, alpha.min = alpha.min, alpha.max = alpha.max, 
                         threads=6)

alpha.hat  <- (exp(fit$par[1]) / (1 + exp(fit$par[1]))) * 
               alpha.rng + alpha.min
if (alpha.hat < 0.3) { 
  alpha.hat <- 0.3
} else if (alpha.hat > 0.9) {
  alpha.hat <- 0.9
}
rho.hat    <- exp(fit$par[2])
xibeta.hat <- fit$par[3:4]
xibeta.var <- solve(fit$hessian[3:4, 3:4])
```

From the pairwise likelihood, we'll be using $\rho = `r round(fit$par[2], 4)`$.
The estimates for the other parameters are $\widehat{\alpha} = `r round(fit$par[1], 3)`$, $\widehat{\xi} = `r round(fit$par[2], 3)`$, and $\widehat{\beta}_0 = `r round(fit$par[3], 3)`$.

```{r plotdata2-2, echo=FALSE, fig.width=4, fig.height=4}
plot(knots.t, ylim = c(0, 1), xlim = c(0, 1), 
     main = "simulated dataset", xlab="", ylab="")
train1.idx <- which(y[obs] == 1)
test1.idx <- which(y[!obs] == 1) + ntrain  # to get to the testing
points(s[train1.idx, ], pch = 21, col = "dodgerblue4", bg = "dodgerblue1")
points(s[test1.idx, ], pch = 21, col = "firebrick4", bg = "firebrick1")
```
 
```{r logit2-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial logit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.logit <- spGLM(formula = y.o ~ 1, family = "binomial", coords = s.o,
                   knots = knots, starting = starting, tuning = tuning,
                   priors = priors, cov.model = cov.model,
                   n.samples = iters, verbose = verbose,
                   n.report = n.report)
```
 
```{r probit2-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial probit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.probit <- probit(Y = y.o, X = X.o, s = s.o, knots = knots,
                     iters = iters, burn = burn, update = update)
```
 
```{r gev2-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                knots = knots, beta.tune = 1, xi.tune = 0.1,
                alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                beta.attempts = 200, xi.attempts = 200,
                alpha.attempts = 7500, rho.attempts = 100,
                spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                alpha.init = alpha.hat, a.init = 10000, iterplot = FALSE,
                alpha.fix = FALSE, rho.fix = TRUE, xibeta.joint = TRUE,
                xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev2-2-g1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g1 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.3, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev2-2-g2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g2 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.5, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev2-2-g3, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g3 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.7, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

## MCMC Results

Here are the iteration plots from the GEV model. 
The true values are $\beta_0 = `r round(-data$thresh, 3)`$, and $\xi = `r xi.t`$.
This is using $\widehat{\alpha}$ and $\widehat{\rho}$ from the PCL fit.

```{r gev1ters2-2, echo=FALSE, fig.width=7, fig.height=2}
par(mfrow=c(1, 2))
plot(fit.gev$beta, type="l", main=bquote(beta[0]))
plot(fit.gev$xi, type="l", main=bquote(xi))
```

```{r resultslogit2-2, echo=FALSE, include=FALSE, cache=TRUE}
yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.p,
                       pred.covars = X.p, start = burn + 1, end = iters,
                       thin = 1, verbose = TRUE, n.report = 500)

post.prob.log <- t(yp.sp.log$p.y.predictive.samples)
bs.log2.2        <- BrierScore(post.prob.log, y.validate)
```

```{r resultsprobit2-2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,
                             s.pred = s.p, knots = knots,
                             start = 1, end = iters - burn, update = 500)
bs.pro2.2 <- BrierScore(post.prob.pro, y.validate)
```

```{r resultsgev2-2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev2-2-g1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g1, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.2.g1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev2-2-g2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g2, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.2.g2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev2-2-g3, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g3, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev2.2.g3 <- BrierScore(post.prob.gev, y.validate)
```

### Dataset 3

```{r generate3-2, echo=FALSE, include=FALSE}
data.seed <- data.seed + 1
set.seed(data.seed)  # data
data <- rRareBinarySpat(x, s = s, knots = knots.t, beta = 0, xi = xi.t,
                        alpha = alpha.t, rho = rho.t, prob.success = prop[2])
y <- data$y

ntrain <- floor(0.75 * ns)
ntest  <- ns - ntrain
obs   <- c(rep(T, ntrain), rep(F, ntest))
y.o   <- y[obs, , drop = FALSE]
X.o   <- matrix(x[obs], ntrain, 1)
s.o   <- s[obs, ]
dw2.o <- rdist(s.o, knots)
d.o   <- as.matrix(rdist(s.o))
diag(d.o) <- 0
y.validate <- y[!obs, , drop = FALSE]
X.p <- matrix(x[!obs, ], ntest, 1)
s.p <- s[!obs, ]
```

```{r fit3-2, echo=FALSE, include=FALSE, cache=TRUE}
fit <- fit.rarebinaryCPP(c(0, rho.init, 0, -4), y = y.o, dw2 = dw2.o, d = d.o, 
                         cov=X.o, alpha.min = alpha.min, alpha.max = alpha.max, 
                         threads=6)

alpha.hat  <- (exp(fit$par[1]) / (1 + exp(fit$par[1]))) * 
               alpha.rng + alpha.min
if (alpha.hat < 0.3) { 
  alpha.hat <- 0.3
} else if (alpha.hat > 0.9) {
  alpha.hat <- 0.9
}
rho.hat    <- exp(fit$par[2])
xibeta.hat <- fit$par[3:4]
xibeta.var <- solve(fit$hessian[3:4, 3:4])
```

From the pairwise likelihood, we'll be using $\rho = `r round(fit$par[2], 4)`$.
The estimates for the other parameters are $\widehat{\alpha} = `r round(fit$par[1], 3)`$, $\widehat{\xi} = `r round(fit$par[2], 3)`$, and $\widehat{\beta}_0 = `r round(fit$par[3], 3)`$.

```{r plotdata3-2, echo=FALSE, fig.width=4, fig.height=4}
plot(knots.t, ylim = c(0, 1), xlim = c(0, 1), 
     main = "simulated dataset", xlab="", ylab="")
train1.idx <- which(y[obs] == 1)
test1.idx <- which(y[!obs] == 1) + ntrain  # to get to the testing
points(s[train1.idx, ], pch = 21, col = "dodgerblue4", bg = "dodgerblue1")
points(s[test1.idx, ], pch = 21, col = "firebrick4", bg = "firebrick1")
```
 
```{r logit3-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial logit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.logit <- spGLM(formula = y.o ~ 1, family = "binomial", coords = s.o,
                   knots = knots, starting = starting, tuning = tuning,
                   priors = priors, cov.model = cov.model,
                   n.samples = iters, verbose = verbose,
                   n.report = n.report)
```
 
```{r probit3-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial probit
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.probit <- probit(Y = y.o, X = X.o, s = s.o, knots = knots,
                     iters = iters, burn = burn, update = update)
```
 
```{r gev3-2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                knots = knots, beta.tune = 1, xi.tune = 0.1,
                alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                beta.attempts = 200, xi.attempts = 200,
                alpha.attempts = 7500, rho.attempts = 100,
                spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                alpha.init = alpha.hat, a.init = 10000, iterplot = FALSE,
                alpha.fix = FALSE, rho.fix = TRUE, xibeta.joint = TRUE,
                xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev3-2-g1, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g1 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.3, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev3-2-g2, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g2 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.5, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

```{r gev3-2-g3, echo = FALSE, include = FALSE, cache = TRUE}
# spatial GEV
mcmc.seed <- mcmc.seed + 1
set.seed(mcmc.seed)
fit.gev.g3 <- mcmc(y = y.o, s = s.o, x = X.o, s.pred = NULL, x.pred = NULL,
                   beta.init = fit$par[4], beta.m = 0, beta.s = 100,
                   xi.init = fit$par[3], xi.m = 0, xi.s = 0.5,
                   knots = knots, beta.tune = 1, xi.tune = 0.1,
                   alpha.tune = 0.05, rho.tune = 0.1, A.tune = 1,
                   beta.attempts = 200, xi.attempts = 200,
                   alpha.attempts = 7500, rho.attempts = 100,
                   spatial = TRUE, rho.init = rho.hat, rho.upper = 9,
                   alpha.init = 0.7, a.init = 10000, iterplot = FALSE,
                   alpha.fix = TRUE, rho.fix = TRUE, xibeta.joint = TRUE,
                   xibeta.hat = xibeta.hat, xibeta.var = xibeta.var,
                   iters = iters, burn = burn, update = update, thin = 1)
```

## MCMC Results

Here are the iteration plots from the GEV model. 
The true values are $\beta_0 = `r round(-data$thresh, 3)`$, and $\xi = `r xi.t`$.
This is using $\widehat{\alpha}$ and $\widehat{\rho}$ from the PCL fit.

```{r gev1ters3-2, echo=FALSE, fig.width=7, fig.height=2}
par(mfrow=c(1, 2))
plot(fit.gev$beta, type="l", main=bquote(beta[0]))
plot(fit.gev$xi, type="l", main=bquote(xi))
```

```{r resultslogit3-2, echo=FALSE, include=FALSE, cache=TRUE}
yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.p,
                       pred.covars = X.p, start = burn + 1, end = iters,
                       thin = 1, verbose = TRUE, n.report = 500)

post.prob.log <- t(yp.sp.log$p.y.predictive.samples)
bs.log3.2        <- BrierScore(post.prob.log, y.validate)
```

```{r resultsprobit3-2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,
                             s.pred = s.p, knots = knots,
                             start = 1, end = iters - burn, update = 500)
bs.pro3.2 <- BrierScore(post.prob.pro, y.validate)
```

```{r resultsgev3-2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev3-2-g1, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g1, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.2.g1 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev3-2-g2, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g2, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.2.g2 <- BrierScore(post.prob.gev, y.validate)
```

```{r resultsgev3-2-g3, echo=FALSE, include=FALSE, cache=TRUE}
post.prob.gev <- pred.spgev(mcmcoutput = fit.gev.g3, x.pred = X.p,
                            s.pred = s.p, knots = knots,
                            start = 1, end = iters - burn, update = 500)
bs.gev3.2.g3 <- BrierScore(post.prob.gev, y.validate)
```

## Brier Scores

The brier scores are 

Logit 1-2: $`r round(bs.log1.2 * 1000, 2)`$

Probit 1-2: $`r round(bs.pro1.2 * 1000, 2)`$

GEV 1-2: $`r round(bs.gev1.2 * 1000, 2)`$

GEV 1-2-g1: $`r round(bs.gev1.2.g1 * 1000, 2)`$

GEV 1-2-g2: $`r round(bs.gev1.2.g2 * 1000, 2)`$

GEV 1-2-g3: $`r round(bs.gev1.2.g3 * 1000, 2)`$

The brier scores are 

Logit 2-2: $`r round(bs.log2.2 * 1000, 2)`$

Probit 2-2: $`r round(bs.pro2.2 * 1000, 2)`$

GEV 2-2: $`r round(bs.gev2.2 * 1000, 2)`$

GEV 2-2-g1: $`r round(bs.gev2.2.g1 * 1000, 2)`$

GEV 2-2-g2: $`r round(bs.gev2.2.g2 * 1000, 2)`$

GEV 2-2-g3: $`r round(bs.gev2.2.g3 * 1000, 2)`$

The brier scores are 

Logit 3-2: $`r round(bs.log3.2 * 1000, 2)`$

Probit 3-2: $`r round(bs.pro3.2 * 1000, 2)`$

GEV 3-2: $`r round(bs.gev3.2 * 1000, 2)`$

GEV 3-2-g1: $`r round(bs.gev3.2.g1 * 1000, 2)`$

GEV 3-2-g2: $`r round(bs.gev3.2.g2 * 1000, 2)`$

GEV 3-2-g3: $`r round(bs.gev3.2.g3 * 1000, 2)`$